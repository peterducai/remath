// extern crate rustc_version_runtime;
// use rustc_version_runtime::version;
// // use cargo_metadata::MetadataCommand;

/// This module makes it easy.
pub mod neural {

    /// Use the abstraction function to do this specific thing.
    pub fn abstraction() {}

}

// Convolutional neural networks (CNNs) 
// contain five types of layers: input, convolution, pooling, fully connected and output. Each layer has a specific purpose, like summarizing, connecting or activating. Convolutional neural networks have popularized image classification and object detection. However, CNNs have also been applied to other areas, such as natural language processing and forecasting.

// Recurrent neural networks (RNNs) 
// use sequential information such as time-stamped data from a sensor device or a spoken sentence, composed of a sequence of terms. Unlike traditional neural networks, all inputs to a recurrent neural network are not independent of each other, and the output for each element depends on the computations of its preceding elements. RNNs are used in foreÂ­casting and time series applications, sentiment analysis and other text applications.

// Feedforward neural networks,
// in which each perceptron in one layer is connected to every perceptron from the next layer. Information is fed forward from one layer to the next in the forward direction only. There are no feedback loops.

// Autoencoder neural networks
// are used to create abstractions called encoders, created from a given set of inputs. Although similar to more traditional neural networks, autoencoders seek to model the inputs themselves, and therefore the method is considered unsupervised. The premise of autoencoders is to desensitize the irrelevant and sensitize the relevant. As layers are added, further abstractions are formulated at higher layers (layers closest to the point at which a decoder layer is introduced). These abstractions can then be used by linear or nonlinear classifiers.